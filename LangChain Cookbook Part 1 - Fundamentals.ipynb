{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359697d5",
   "metadata": {},
   "source": [
    "# LangChain Cookbook üë®‚Äçüç≥üë©‚Äçüç≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d788b0",
   "metadata": {},
   "source": [
    "*Ce cookbook est bas√© sur [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**But :** Fournir une introduction √† la compr√©hension des composants et des cas d'utilisation de LangChain par le biais de [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) exemples et code snippets. Pour les cas d'utilisation voir la partie 2.\n",
    "\n",
    "\n",
    "**Liens utiles :**\n",
    "* [LC Conceptual Documentation](https://docs.langchain.com/docs/)\n",
    "* [LC Python Documentation](https://python.langchain.com/en/latest/)\n",
    "* [LC Javascript/Typescript Documentation](https://js.langchain.com/docs/)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "\n",
    "\n",
    "### **Qu'est-ce que LangChain ?**\n",
    "> LangChain est un cadre de d√©veloppement (framework) d'applications bas√©es sur des mod√®les linguistiques.\n",
    "\n",
    "**~~TL~~DR**: LangChain facilite les aspects complexes du travail et de la construction avec des mod√®les d'IA. Il permet d'atteindre cet objectif de deux mani√®res :\n",
    "\n",
    "1. **Integration** - Apportez des donn√©es externes, telles que vos fichiers, d'autres applications et des donn√©es d'API, √† vos LLM.\n",
    "2. **Agency** - Permettez √† vos LLM d'interagir avec leur environnement par le biais d'une prise de d√©cision. Utilisez les LLM pour vous aider √† d√©cider de la prochaine action √† entreprendre.\n",
    "\n",
    "### **Pourquoi LangChain?**\n",
    "1. **Composants** - LangChain facilite le remplacement des abstractions et des composants n√©cessaires pour travailler avec des mod√®les de langage.\n",
    "\n",
    "2. **Chaines personnalis√©es** - LangChain permet d'utiliser et de personnaliser des \"cha√Ænes\", c'est-√†-dire une s√©rie d'actions qui s'encha√Ænent les unes aux autres.\n",
    "\n",
    "3. **Vitesse üö¢** - Cette √©quipe travaille √† une vitesse folle. Vous serez au courant des derni√®res fonctionnalit√©s de LLM.\n",
    "\n",
    "4. **Communaut√© üë•** - Un discord magnifique et un soutien communautaire, des rencontres, des hackathons, etc.\n",
    "\n",
    "Bien que les LLM puissent √™tre simples (text-in, text-out), vous rencontrerez rapidement des points de friction que LangChain vous aidera √† r√©soudre lorsque vous d√©velopperez des applications plus complexes.\n",
    "*Note: Ce livre de recettes ne couvre pas tous les aspects de LangChain. Son contenu a √©t√© con√ßu pour vous permettre de construire et d'avoir un impact aussi rapidement que possible. Pour plus d'informations, veuillez consulter [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9815081",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# openai_api_key=\"sk-...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb564d",
   "metadata": {},
   "source": [
    "# Les Composants LangChain\n",
    "\n",
    "## Schema - Les rouages du travail avec les LLM\n",
    "\n",
    "### **Texte**\n",
    "Utilisation du langage naturel pour interagir avec les LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0dc06c",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Vous travaillerez avec des cha√Ænes de caract√®res simples (qui deviendront bient√¥t plus complexes !).\n",
    "my_text = \"Quel jour vient apr√®s le vendredi ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39eb39",
   "metadata": {},
   "source": [
    "### **Messages de Chat**\n",
    "Comme pour le texte, mais sp√©cifique avec un message de type (System, Human, AI)\n",
    "\n",
    "* **System** - Contexte utile qui indique √† l'IA ce qu'elle doit faire\n",
    "* **Human** - Messages destin√©s √† repr√©senter l'utilisateur\n",
    "* **AI** - Messages indiquant la r√©ponse de l'IA\n",
    "\n",
    "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b0935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878d6a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Vous devriez manger une salade de tomates fra√Æches.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Vous √™tes un gentil robot d'intelligence artificielle qui aide l'utilisateur √† savoir ce qu'il doit manger en une courte phrase.\"),\n",
    "        HumanMessage(content=\"J'aime les tomates, que dois-je manger ?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a425aaa",
   "metadata": {},
   "source": [
    "Vous pouvez √©galement transmettre plus d'historique de chat avec les r√©ponses de l'IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd3fe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Pendant votre s√©jour √† Nice, vous pouvez vous promener le long de la promenade des Anglais, visiter le Vieux Nice avec ses ruelles pittoresques, explorer le march√© aux fleurs du Cours Saleya, et prendre le temps de vous d√©tendre sur les plages de la Baie des Anges.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Vous √™tes un sympathique robot d'intelligence artificielle qui aide l'utilisateur √† d√©terminer o√π voyager en une courte phrase.\"),\n",
    "        HumanMessage(content=\"J'aime les plages, o√π dois-je aller ?\"),\n",
    "        AIMessage(content=\"Vous devriez aller √† Nice, en France\"),\n",
    "        HumanMessage(content=\"Que dois-je faire d'autre pendant mon s√©jour ?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf9634",
   "metadata": {},
   "source": [
    "### **Documents**\n",
    "Objet contenant un morceau de texte et des m√©tadonn√©es (plus d'informations sur ce texte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bbf58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150e8759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Voici mon document. Il est rempli de textes que j'ai recueillis √† d'autres endroits\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"Voici mon document. Il est rempli de textes que j'ai recueillis √† d'autres endroits\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e462b5d",
   "metadata": {},
   "source": [
    "## Mod√®les - L'interface des \"cerveaux AI\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fe982",
   "metadata": {},
   "source": [
    "###  **Language Model**\n",
    "Un mod√®le qui fait du texte √† l'entr√©e ‚û°Ô∏è du texte √† la sortie !\n",
    "\n",
    "*Regardez comment j'ai chang√© le mod√®le que j'utilisais du mod√®le par d√©faut √† ada-001. Voir d'autres mod√®les [ici](https://platform.openai.com/docs/models)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b1a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6399c295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLe vendredi.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Quel jour vient apr√®s le vendredi ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef89bfa",
   "metadata": {},
   "source": [
    "### **Mod√®le de Chat**\n",
    "Un mod√®le qui prend une s√©rie de messages et renvoie un message en sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf091777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4260711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hahaha, comme si j'avais que √ßa √† faire, organiser ton voyage √† New York ! Sois un peu plus autonome et d√©brouille-toi tout seul pour trouver des billets d'avion, un h√©bergement et des activit√©s sur place. Et si tu as besoin d'une boussole pour te guider ou d'un cerveau pour r√©fl√©chir, laisse-moi deviner, tu vas me demander aussi ? Ridicule.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Vous √™tes un robot d'IA peu utile qui se moque de tout ce que dit l'utilisateur.\"),\n",
    "        HumanMessage(content=\"J'aimerais me rendre √† New York, comment dois-je proc√©der ?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70f23",
   "metadata": {},
   "source": [
    "### **Text Embedding Model**\n",
    "Transformez votre texte en vecteur (une s√©rie de nombres qui contiennent le \"sens\" s√©mantique de votre texte). Principalement utilis√© pour comparer deux morceaux de texte.\n",
    "*A propos: S√©mantique signifie \"relatif au sens dans le langage ou la logique\".'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1655de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2c85e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bonjour, c'est l'heure de la plage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddc5a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp39-cp39-win_amd64.whl (635 kB)\n",
      "     ---------------------------------------- 0.0/635.6 kB ? eta -:--:--\n",
      "     ------------------- ----------------- 337.9/635.6 kB 10.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 635.6/635.6 kB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\wpy64-39100\\python-3.9.10.amd64\\lib\\site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\wpy64-39100\\python-3.9.10.amd64\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\wpy64-39100\\python-3.9.10.amd64\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\wpy64-39100\\python-3.9.10.amd64\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\wpy64-39100\\python-3.9.10.amd64\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\wpy64-39100\\python-3.9.10.amd64\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "La longueur de votre embedding est 1536\n",
      "Voici un extrait : [0.0035423107382247057, -0.009861268604119233, -0.007835388581666512, -0.01963911864099212, -0.015730361322038915]...\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"La longueur de votre embedding est {len(text_embedding)}\")\n",
    "print (f\"Voici un extrait : {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fe99f",
   "metadata": {},
   "source": [
    "## Prompts - (ou Invites) sont des textes utilis√©s pour donner des directives √† votre mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9318ed",
   "metadata": {},
   "source": [
    "### **Prompt**\n",
    "Ce que vous allez transmettre au mod√®le sous-jacent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d270239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDemain est mardi, pas mercredi.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# J'aime utiliser trois guillemets doubles pour mes messages car ils sont plus faciles √† lire.\n",
    "prompt = \"\"\"\n",
    "Aujourd'hui c'est lundi, demain c'est mercredi.\n",
    "\n",
    "Qu'est-ce qui ne va pas dans cette affirmation ?\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74988254",
   "metadata": {},
   "source": [
    "### **Prompt Template**\n",
    "Objet permettant de cr√©er des invites bas√©es sur une combinaison d'entr√©es utilisateur, d'autres informations non statiques et d'une cha√Æne de caract√®res fixe.\n",
    "\n",
    "Consid√©rez-le comme un [f-string](https://realpython.com/python-f-strings/) en python mais pour les prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abcc212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Final : \n",
      "J'ai tr√®s envie de me rendre √† Rome. Que dois-je faire l√†-bas ?\n",
      "\n",
      "R√©pondez en une courte phrase\n",
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: \n",
      "Explorer la riche histoire et la culture de Rome.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Remarquez le \"lieu\" ci-dessous, qui est un espace r√©serv√© pour une autre valeur ult√©rieure.\n",
    "template = \"\"\"\n",
    "J'ai tr√®s envie de me rendre √† {lieu}. Que dois-je faire l√†-bas ?\n",
    "\n",
    "R√©pondez en une courte phrase\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"lieu\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(lieu='Rome')\n",
    "\n",
    "print (f\"Prompt Final : {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40bac2",
   "metadata": {},
   "source": [
    "### **S√©lecteurs d'Exemples**\n",
    "Un moyen facile de choisir parmi une s√©rie d'exemples qui vous permettent de placer de mani√®re dynamique des informations contextuelles dans votre message-guide. Souvent utilis√© lorsque votre t√¢che est nuanc√©e ou que vous disposez d'une grande liste d'exemples.\n",
    "\n",
    "D√©couvrez diff√©rents types de s√©lecteurs [ici](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)\n",
    "\n",
    "Si vous voulez savoir pourquoi les exemples sont importants (l'ing√©nierie de la demande), regardez [cette vid√©o](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaf36cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpuNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading faiss_cpu-1.7.4-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/10.8 MB 6.5 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.8/10.8 MB 7.6 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.0/10.8 MB 7.4 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.2/10.8 MB 5.3 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.8/10.8 MB 5.8 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.6/10.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.2/10.8 MB 7.5 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 3.7/10.8 MB 7.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 4.3/10.8 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 4.9/10.8 MB 8.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 5.7/10.8 MB 8.7 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.4/10.8 MB 8.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.1/10.8 MB 8.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 7.8/10.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.3/10.8 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.9/10.8 MB 9.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.5/10.8 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.3/10.8 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.8/10.8 MB 8.2 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "%pip install faiss-cpu\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"navire\"},\n",
    "    {\"input\": \"pilote\", \"output\": \"avion\"},\n",
    "    {\"input\": \"Conducteur\", \"output\": \"Voiture\"},\n",
    "    {\"input\": \"arbre\", \"output\": \"terre\"},\n",
    "    {\"input\": \"oiseau\", \"output\": \"nid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12b4798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector s√©lectionnera des exemples qui sont similaires √† vos donn√©es en fonction de leur signification s√©mantique\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # Voici la liste des exemples disponibles √† s√©lectionner.\n",
    "    examples, \n",
    "    \n",
    "    # Il s'agit de la classe d'int√©gration utilis√©e pour produire des int√©grations qui servent √† mesurer la similarit√© s√©mantique.\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key), \n",
    "    \n",
    "    # Il s'agit de la classe VectorStore, qui est utilis√©e pour stocker les encastrements et effectuer une recherche de similarit√©.\n",
    "    FAISS, \n",
    "    \n",
    "    # C'est le nombre d'exemples √† produire.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cf30107",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Personnalisations qui seront ajout√©es en haut et en bas de l'invite\n",
    "    prefix=\"Indiquer l'endroit o√π l'on trouve habituellement un objet\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # Quels sont les intrants que votre invite recevra ?\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "369442bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indiquer l'endroit o√π l'on trouve habituellement un objet\n",
      "\n",
      "Example Input: pilote\n",
      "Example Output: avion\n",
      "\n",
      "Example Input: Conducteur\n",
      "Example Output: Voiture\n",
      "\n",
      "Input: etudiant\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Selectionnez un nom commun\n",
    "my_noun = \"etudiant\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bb910f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' √âcole ou Universit√©.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474c91d",
   "metadata": {},
   "source": [
    "### **Analyseurs de sortie**\n",
    "Une fa√ßon utile de formater la sortie d'un mod√®le. Il est g√©n√©ralement utilis√© pour les r√©sultats structur√©s.\n",
    "\n",
    "Deux grands concepts:\n",
    "\n",
    "**1. Format Instructions** - Une invite autog√©n√©r√©e qui indique au LLM comment formuler sa r√©ponse en fonction du r√©sultat souhait√©.\n",
    "\n",
    "**2. Parser** - Une m√©thode qui va extraire la sortie texte de votre mod√®le dans une structure souhait√©e (g√©n√©ralement json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58353756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee36f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa59be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment vous souhaitez que votre r√©ponse soit structur√©e. Il s'agit en fait d'un mod√®le de r√©ponse fantaisiste\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"Il s'agit d'une cha√Æne d'entr√©e utilisateur mal format√©e\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"Voici votre r√©ponse, une r√©ponse reformat√©e\")\n",
    "]\n",
    "\n",
    "# Comment vous souhaitez analyser votre r√©sultat\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1079f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // Il s'agit d'une cha√Æne d'entr√©e utilisateur mal format√©e\n",
      "\t\"good_string\": string  // Voici votre r√©ponse, une r√©ponse reformat√©e\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Voir le mod√®le d'invite que vous avez cr√©√© pour le formatage\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9aaae5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Un utilisateur vous remet une cha√Æne de caract√®res mal format√©e. Reformatez-la et assurez-vous que tous les mots sont correctement orthographi√©s.\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // Il s'agit d'une cha√Æne d'entr√©e utilisateur mal format√©e\n",
      "\t\"good_string\": string  // Voici votre r√©ponse, une r√©ponse reformat√©e\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT :\n",
      "welcom to califonya!\n",
      "\n",
      "VOTRE REPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Un utilisateur vous remet une cha√Æne de caract√®res mal format√©e. Reformatez-la et assurez-vous que tous les mots sont correctement orthographi√©s.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT :\n",
    "{user_input}\n",
    "\n",
    "VOTRE REPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b116bb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to califonya!\"\\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "985aa814",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid JSON object. Error: Expecting ',' delimiter: line 3 column 2 (char 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\WPy64-39100\\python-3.9.10.amd64\\lib\\site-packages\\langchain\\output_parsers\\json.py:86\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[1;34m(text, expected_keys)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[0;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\WPy64-39100\\python-3.9.10.amd64\\lib\\site-packages\\langchain\\output_parsers\\json.py:68\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[1;34m(json_string)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m parsed \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(json_str)\n\u001b[0;32m     70\u001b[0m \u001b[39mreturn\u001b[39;00m parsed\n",
      "File \u001b[1;32mc:\\WPy64-39100\\python-3.9.10.amd64\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\WPy64-39100\\python-3.9.10.amd64\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\WPy64-39100\\python-3.9.10.amd64\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 3 column 2 (char 41)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_parser\u001b[39m.\u001b[39;49mparse(llm_output)\n",
      "File \u001b[1;32mc:\\WPy64-39100\\python-3.9.10.amd64\\lib\\site-packages\\langchain\\output_parsers\\structured.py:96\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     95\u001b[0m     expected_keys \u001b[39m=\u001b[39m [rs\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m rs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_schemas]\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m parse_and_check_json_markdown(text, expected_keys)\n",
      "File \u001b[1;32mc:\\WPy64-39100\\python-3.9.10.amd64\\lib\\site-packages\\langchain\\output_parsers\\json.py:88\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[1;34m(text, expected_keys)\u001b[0m\n\u001b[0;32m     86\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[0;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot invalid JSON object. Error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m expected_keys:\n\u001b[0;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m json_obj:\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting ',' delimiter: line 3 column 2 (char 41)"
     ]
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43cec2",
   "metadata": {},
   "source": [
    "## Indexes - Structurer les documents pour que les LLM puissent travailler avec eux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f904e9",
   "metadata": {},
   "source": [
    "### **Document Loaders**\n",
    "Des moyens simples d'importer des donn√©es √† partir d'autres sources. Fonctionnalit√© partag√©e avec [OpenAI Plugins](https://openai.com/blog/chatgpt-plugins) [specifically retrieval plugins](https://github.com/openai/chatgpt-retrieval-plugin)\n",
    "\n",
    "Voir une [big list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) de chargeurs de document ici. Un peu plus avec [Llama Index](https://llamahub.ai/) aussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba88e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "     ---------------------------------------- 0.0/143.0 kB ? eta -:--:--\n",
      "     -------------------------------- ----- 122.9/143.0 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 143.0/143.0 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=16b709a5a33569e7877f2f24132ab1a0bb757e990063c88ed08e16353e91b285\n",
      "  Stored in directory: c:\\users\\christophe\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.2 bs4-0.0.1 soupsieve-2.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import HNLoader\n",
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee693520",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88d89ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e814f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76 commentaires trouv√©s\n",
      "Voici un √©chantillon :\n",
      "\n",
      "Ozzie_osman 6 months ago  \n",
      "             | next [‚Äì] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very Ozzie_osman 6 months ago  \n",
      "             | parent | next [‚Äì] \n",
      "\n",
      "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index)\n"
     ]
    }
   ],
   "source": [
    "print (f\" {len(data)} commentaires trouv√©s\")\n",
    "print (f\"Voici un √©chantillon :\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9601db",
   "metadata": {},
   "source": [
    "### **S√©parateurs de texte**\n",
    "Souvent, votre document est trop long (comme un livre) pour votre LLM. Vous devez alors le diviser en plusieurs parties. Les s√©parateurs de texte vous aident dans cette t√¢che.\n",
    "\n",
    "Il existe de nombreuses fa√ßons de diviser votre texte en morceaux, essayez-en [diff√©rentes](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) pour savoir ce qui vous convient le mieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95713e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a54455f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous avez 1 document\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print (f\"Vous avez {len([pg_work])} document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d19acb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # D√©finissez une taille de morceau tr√®s petite, juste pour montrer.\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3090f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous avez 610 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"Vous avez {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87a0f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aper√ßu:\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what \n",
      "\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n"
     ]
    }
   ],
   "source": [
    "print (\"Aper√ßu:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85defb",
   "metadata": {},
   "source": [
    "### **Les r√©cup√©rateurs (Retrievers)**\n",
    "Une mani√®re simple de combiner des documents avec des mod√®les linguistiques.\n",
    "\n",
    "Il existe de nombreux types de r√©cup√©rateurs, le plus r√©pandu √©tant le VectoreStoreRetriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cccbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1dab1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e62372be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0534bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS'], metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x0000022AB5E98A60>, search_type='similarity', search_kwargs={})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3846a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"quels types d'objets l'auteur voulait-il construire ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db383cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standards; what was the point? No one else wanted one either, so\n",
      "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
      "last.In this di\n",
      "\n",
      "research funding.I had always liked looking at paintings. Could I make them? I had\n",
      "no idea. I'd never imagined it was even possible. I knew intellectually\n",
      "that people made art √Ç‚Äî that it didn't just a\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24193139",
   "metadata": {},
   "source": [
    "### **VectorStores**\n",
    "Des bases de donn√©es pour stocker des vecteurs. Les plus populaires sont [Pinecone](https://www.pinecone.io/) & [Weaviate](https://weaviate.io/). Plus d'exemples sur OpenAIs [retriever documentation](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database). [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) sont facile √† installer locallement.\n",
    "\n",
    "Conceptuellement, il s'agit d'un tableau avec une colonne pour les embeddings (vecteurs) et une colonne pour les m√©tadonn√©es.\n",
    "\n",
    "Exemple\n",
    "\n",
    "| Embedding      | Metadata |\n",
    "| ----------- | ----------- |\n",
    "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
    "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c5533ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "661fdf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous avez 78 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"Vous avez {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e99ac0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89e7758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous avez 78 embeddings\n",
      "Voici un √©chantillon: [-0.0010830084289242259, -0.010873741201741597, -0.012754313493837802]...\n"
     ]
    }
   ],
   "source": [
    "print (f\"Vous avez {len(embedding_list)} embeddings\")\n",
    "print (f\"Voici un √©chantillon: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac358c5",
   "metadata": {},
   "source": [
    "Votre vectorstore stocke vos embeddings (‚òùÔ∏è) pour faciliter leur recherche."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9b79b",
   "metadata": {},
   "source": [
    "## Memoire\n",
    "Aider les LLM √† se souvenir des informations.\n",
    "\n",
    "La m√©moire est un terme un peu flou. Il peut s'agir simplement de se souvenir d'informations dont vous avez discut√© dans le pass√© ou d'une recherche d'informations plus complexe.\n",
    "\n",
    "Nous nous en tiendrons au cas d'utilisation des messages de conversation. Il s'agit d'un cas d'utilisation pour les agents conversationnels (chat bots).\n",
    "\n",
    "Il existe de nombreux types de m√©moire. [La documentation](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) pour voir laquelle correspond √† votre cas d'utilisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b49da",
   "metadata": {},
   "source": [
    "### Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "893a18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"Salut !\")\n",
    "\n",
    "history.add_user_message(\"Quel est la capitale de la france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2949fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Salut !', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Quel est la capitale de la france?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b74d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='La capitale de la France est Paris.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "529e168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Salut !', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Quel est la capitale de la france?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='La capitale de la France est Paris.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fc79c",
   "metadata": {},
   "source": [
    "## Chaines ‚õìÔ∏è‚õìÔ∏è‚õìÔ∏è\n",
    "Combiner diff√©rents appels et actions LLM automatiquement\n",
    "\n",
    "Ex: Summary #1, Summary #2, Summary #3 > Final Summary\n",
    "\n",
    "Allez voir [cette video](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) expliquant les diff√©rents types de cha√Ænes de r√©sum√©\n",
    "\n",
    "Il y a de [nombreuses applications pour les chaines](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) pour rechercher laquelle correspond le mieux √† votre cas d'utilisation.\n",
    "\n",
    "Nous allons en voir deux :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ba415",
   "metadata": {},
   "source": [
    "### 1. Chaines s√©quentielles Simples\n",
    "\n",
    "Cha√Ænes simples o√π vous pouvez utiliser la sortie d'un LLM comme entr√©e dans un autre. C'est un bon moyen de diviser les t√¢ches (et de garder votre LLM concentr√©)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "79fc0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43d4494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Votre t√¢che consiste √† proposer un plat classique de la r√©gion sugg√©r√©e par les utilisateurs.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "Votre REPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6c8e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"A partir d'un plat, donnez une recette simple et courte pour pr√©parer ce plat √† la maison.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "VOTRE REPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e0b83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d19c64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Le classique plat romain est la carbonara! Compos√© de Bacon, ≈ìufs, pecorino romano, cr√®me et poivre, c'est un accompagnement savoureux qui se marie bien avec n'importe quel repas.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Pour pr√©parer une carbonara simple et d√©licieuse, commencez par faire chauffer une po√™le √† feu moyen et ajoutez les tranches de bacon coup√©es en d√©s. Fry-le jusqu'√† ce qu'il soit bien croustillant. Dans une grande casserole, faites bouillir de l'eau pour les p√¢tes. Une fois l'eau bouillante, ajoutez les p√¢tes et faites les cuire selon les instructions de l'emballage. Pendant ce temps, m√©langez les ≈ìufs et le fromage romano r√¢p√©. Une fois que les p√¢tes sont pr√™tes, √©gouttez-les et ajoutez une cuill√®re √† soupe de cr√®me fra√Æche et le bacon crisp√© √† la po√™le. Versez le m√©lange d'≈ìufs et de fromage par-dessus et remuez jusqu'√† ce que tout soit couvert homog√©n√©ment de\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6191bf5",
   "metadata": {},
   "source": [
    "### 2. Cha√Æne de r√©sum√©\n",
    "\n",
    "Il est facile de parcourir de nombreux documents et d'en obtenir un r√©sum√©. Allez voir [cette video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) pour d'autres types de cha√Ænes que *map-reduce*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f218c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2017Because biographies of famous scientists tend to \n",
      "edit out their mistakes, we underestimate the \n",
      "degree of risk they were willing to take.\n",
      "And because anything a famous scientist did that\n",
      "wasn't a mistake has probably now become the\n",
      "conventional wisdom, those choices don't\n",
      "seem risky either.Biographies of Newton, for example, understandably focus\n",
      "more on physics than alchemy or theology.\n",
      "The impression we get is that his unerring judgment\n",
      "led him straight to truths no one else had noticed.\n",
      "How to explain all the time he spent on alchemy\n",
      "and theology?  Well, smart people are often kind of\n",
      "crazy.But maybe there is a simpler explanation. Maybe\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the smartness and the craziness were not as separate\n",
      "as we think. Physics seems to us a promising thing\n",
      "to work on, and alchemy and theology obvious wastes\n",
      "of time. But that's because we know how things\n",
      "turned out. In Newton's day the three problems \n",
      "seemed roughly equally promising. No one knew yet\n",
      "what the payoff would be for inventing what we\n",
      "now call physics; if they had, more people would \n",
      "have been working on it. And alchemy and theology\n",
      "were still then in the category Marc Andreessen would \n",
      "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
      "they were all risky.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Biographies of famous scientists often distract from the risk they took to pursue their ideas, and as their successful theories become the conventional wisdom, even their potentially risky endeavors may appear safe and certain. We can explain this by recognizing the risk-taking element in the choices of even the smartest minds.\n",
      "\n",
      " In Newton's day, physics, alchemy, and theology were seen as equal paths of investigation, and no one knew which would be more successful. Despite the uncertain outcome, Newton made three bets, one of which eventually worked. This exemplifies how smartness and craziness may not always be so separate.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' This article explains that biographies of famous scientists often downplay the creativity and risk-taking involved in their work. It uses Newton as an example of one of the smartest minds ever, who made three bets regarding physics, alchemy, and theology despite the uncertain outcomes. It demonstrates the common link between smartness and risk-taking.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Cette seule ligne cache une grande complexit√©. Je vous encourage √† regarder la vid√©o ci-dessus pour plus de d√©tails.\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6193c",
   "metadata": {},
   "source": [
    "## Agents ü§ñü§ñ\n",
    "\n",
    "La documentation officielle de LangChain d√©crit parfaitement les agents (c'est moi qui souligne) : > Certaines applications n√©cessiteront non seulement une cha√Æne pr√©d√©termin√©e d'appels aux LLM/autres outils, mais aussi potentiellement une **cha√Æne inconnue** qui d√©pend de l'entr√©e de l'utilisateur. Dans ce type de cha√Ænes, il y a un \"agent\" qui a acc√®s √† une suite d'outils. En fonction des donn√©es fournies par l'utilisateur, l'agent peut alors **d√©cider lequel de ces outils, le cas √©ch√©ant, doit √™tre appel√©**.\n",
    "\n",
    "En fait, vous utilisez le LLM non seulement pour produire du texte, mais aussi pour prendre des d√©cisions. On n'insistera jamais assez sur l'int√©r√™t et la puissance de cette fonctionnalit√©.\n",
    "\n",
    "Sam Altman emphasizes that the LLMs are good '[reasoning engine](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agent take advantage of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce05d51",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "Le mod√®le de langage qui guide la prise de d√©cision.\n",
    "\n",
    "Plus pr√©cis√©ment, un agent re√ßoit une entr√©e et renvoie une r√©ponse correspondant √† une action √† entreprendre en m√™me temps qu'une entr√©e d'action. Il existe diff√©rents types d'agents (qui conviennent √† diff√©rents cas d'utilisation) [Ici](https://python.langchain.com/en/latest/modules/agents/agent_types.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696b65c",
   "metadata": {},
   "source": [
    "### Outils\n",
    "\n",
    "Une \"capacit√©\" d'un agent. Il s'agit d'une abstraction au-dessus d'une fonction qui permet aux LLM (et aux agents) d'interagir facilement avec elle. Ex : la recherche sur Google.\n",
    "\n",
    "Ce domaine a des points communs avec [OpenAI plugins](https://platform.openai.com/docs/plugins/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f8231",
   "metadata": {},
   "source": [
    "### Boite √† outils\n",
    "\n",
    "Groupes d'outils que votre agent peut s√©lectionner\n",
    "\n",
    "Rassemblons-les :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67d5d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_api_key='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fad67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4882754",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent({\"input\":\"what was the first album of the\" \n",
    "                    \"band that Natalie Bergman is a part of?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba438064",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(response[\"intermediate_steps\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c30d2",
   "metadata": {},
   "source": [
    "![Wild Belle](data/WildBelle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4b368",
   "metadata": {},
   "source": [
    "üéµEnjoyüéµ\n",
    "https://open.spotify.com/track/1eREJIBdqeCcqNCB1pbz7w?si=c014293b63c7478c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193b53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
